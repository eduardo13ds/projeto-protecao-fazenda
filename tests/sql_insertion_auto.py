import pandas as pd
import numpy as np
import mysql.connector
import sys
from contextlib import contextmanager
from datetime import datetime

# --- CONFIGURA√á√ïES ---
# Caminho para o seu arquivo CSV do INMET
import os
CSV_FILE_PATH = os.path.abspath('generatedBy_react-csv (22).csv')
# ID da Fazenda para a qual os sensores pertencem
# (com base no seu schema, os sensores INMET est√£o associados √† ID_Fazenda = 1)
ID_FAZENDA = 1

# Configura√ß√£o do banco de dados
DB_CONFIG = {
    'user': 'root',
    'password': '',
    'host': 'localhost',
    'port': 3306,
    'database': 'stormguard',
    'autocommit': False,
    'buffered': True,
    'consume_results': True,
    'connection_timeout': 30
}

# --- MAPEAMENTO DE COLUNAS DO CSV PARA IDs DOS SENSORES ---
# Este dicion√°rio traduz os nomes das colunas do CSV para o ID_Sensor correspondente no seu banco de dados.
# Verifique se os nomes das colunas aqui correspondem exatamente aos do seu arquivo CSV.
# Os IDs dos sensores (valores) foram retirados da sua tabela `sensores`.
SENSOR_ID_MAP = {
    'Temp. Ins. (C)': 47,  # Temperatura instant√¢nea
    'Umi. Ins. (%)': 48,   # Umidade instant√¢nea
    'Pto Orvalho Ins. (C)': 41,  # Ponto de orvalho instant√¢neo
    'Pressao Ins. (hPa)': 40,    # Press√£o instant√¢nea
    'Vel. Vento (m/s)': 42,      # Velocidade do vento
    'Dir. Vento (m/s)': 43,      # Dire√ß√£o do vento
    'Raj. Vento (m/s)': 44,      # Rajada do vento
    'Radiacao (KJ/m√Ç¬≤)': 45      # Radia√ß√£o
}

class DatabaseManager:
    def __init__(self, config):
        self.config = config

    @contextmanager
    def get_connection(self):
        """Context manager for database connections"""
        conn = None
        try:
            conn = mysql.connector.connect(**self.config)
            conn.autocommit = False
            yield conn
        except mysql.connector.Error as e:
            print(f"Erro ao conectar com MariaDB: {e}")
            if conn:
                conn.close()
            sys.exit(1)
        finally:
            if conn and conn.is_connected():
                conn.close()

    def execute_batch_insert(self, query, data_batch):
        """Execute batch insert for better performance"""
        with self.get_connection() as conn:
            cursor = None
            try:
                cursor = conn.cursor(buffered=True)
                cursor.executemany(query, data_batch)
                conn.commit()
                return cursor.rowcount
            except mysql.connector.Error as e:
                print(f"Erro ao executar batch insert: {e}")
                conn.rollback()
                return 0
            finally:
                if cursor:
                    cursor.close()

    def test_connection(self):
        """Test database connection"""
        try:
            with self.get_connection() as conn:
                cursor = None
                try:
                    cursor = conn.cursor(buffered=True)
                    cursor.execute("SELECT 1 as test")
                    result = cursor.fetchall()  # Consume all results
                    print("‚úì Conex√£o com banco de dados testada com sucesso!")
                    return True
                finally:
                    if cursor:
                        cursor.close()
        except Exception as e:
            print(f"‚úó Erro na conex√£o com banco de dados: {e}")
            return False

    def check_duplicate_records(self, sensor_id, timestamp_start, timestamp_end):
        """Check if there are existing records for a sensor in the given time range"""
        query = """
        SELECT COUNT(*) FROM registro_leituras
        WHERE ID_Sensor = %s AND Timestamp_Leitura BETWEEN %s AND %s
        """
        with self.get_connection() as conn:
            cursor = None
            try:
                cursor = conn.cursor(buffered=True)
                cursor.execute(query, (sensor_id, timestamp_start, timestamp_end))
                result = cursor.fetchone()
                count = result[0] if result else 0
                return count
            finally:
                if cursor:
                    cursor.close()

def processar_dados_e_inserir_no_banco():
    """
    Fun√ß√£o principal que l√™ o CSV, processa todos os dados e insere
    diretamente no banco de dados MariaDB.
    """

    # Inicializar o gerenciador de banco de dados
    db_manager = DatabaseManager(DB_CONFIG)

    print("üîÑ Testando conex√£o com banco de dados...")
    # Testar conex√£o
    if not db_manager.test_connection():
        print("Abortando devido a problemas de conex√£o com o banco de dados.")
        return

    try:
        # Tenta ler o arquivo CSV.
        # skiprows=8: Pula as 8 linhas de cabe√ßalho do arquivo do INMET.
        # sep=';': Define o ponto e v√≠rgula como separador de colunas.
        # decimal=',': Define a v√≠rgula como separador decimal.
        # encoding='latin-1': Codifica√ß√£o comum para arquivos gerados no Brasil.
        print(f"üìÑ Tentando ler arquivo: {CSV_FILE_PATH}")
        df = pd.read_csv(CSV_FILE_PATH, sep=';', decimal=',', encoding='latin-1')
        print(f"‚úì Arquivo '{CSV_FILE_PATH}' lido com sucesso.")
        print(f"‚úì Total de linhas no arquivo: {len(df)}")

    except FileNotFoundError:
        print(f"‚úó ERRO: O arquivo '{CSV_FILE_PATH}' n√£o foi encontrado.")
        print("Por favor, verifique se o nome e o caminho do arquivo est√£o corretos.")
        return
    except Exception as e:
        print(f"‚úó Ocorreu um erro inesperado ao ler o arquivo: {e}")
        return

    # --- LIMPEZA E PREPARA√á√ÉO DOS DADOS ---

    # Remove a √∫ltima coluna, que geralmente est√° vazia ou √© in√∫til nos arquivos do INMET
    df = df.iloc[:, :-1]

    # Substitui os valores -9999 (padr√£o do INMET para dados ausentes) por NaN (Not a Number)
    df.replace(-9999, np.nan, inplace=True)

    print(f"üìä Colunas originais do CSV: {list(df.columns)}")

    # Renomeia colunas para facilitar o acesso
    # Remove os caracteres BOM no in√≠cio da primeira coluna
    df.columns = df.columns.str.replace('√Ø¬ª¬ø"', '', regex=False)
    df.columns = df.columns.str.replace('"', '', regex=False)
    
    df.rename(columns={
        'Data': 'data',
        'Hora (UTC)': 'hora',
    }, inplace=True)

    # Remove o sufixo ' (UTC)' da coluna de hora, se existir
    if 'hora' in df.columns:
        df['hora'] = df['hora'].astype(str).str.replace(' (UTC)', '', regex=False).str.zfill(4)
    else:
        print("‚úó Coluna 'hora' n√£o encontrada ap√≥s renomea√ß√£o. Verifique o CSV.")
        return

    # Renomeia as colunas de dados para remover espa√ßos e caracteres especiais para facilitar o acesso
    # Ex: 'PRECIPITACAO TOTAL, HORARIO (mm)' vira 'PRECIPITACAO_TOTAL_HORARIO_mm'
    df.columns = [col.upper().replace(' (¬∞ (GR))', '').replace(' (¬∞C)', '').replace(' (KJ/M¬≤)', '').replace(' (M/S)', '').replace(' (MM)', '').replace(' (MB)', '').replace(' (%)', '').replace(',', '').replace(' ', '_').replace('-', '_') for col in df.columns]

    # Atualiza as chaves do dicion√°rio de mapeamento para corresponder √†s colunas renomeadas
    cleaned_sensor_map = {key.upper().replace(' (¬∞ (GR))', '').replace(' (¬∞C)', '').replace(' (KJ/M¬≤)', '').replace(' (M/S)', '').replace(' (MM)', '').replace(' (MB)', '').replace(' (%)', '').replace(',', '').replace(' ', '_').replace('-', '_'): value for key, value in SENSOR_ID_MAP.items()}

    print(f"üîó Mapeamento de sensores:")
    for col_name, sensor_id in cleaned_sensor_map.items():
        if col_name in df.columns:
            print(f"   ‚úì {col_name} -> Sensor ID {sensor_id}")
        else:
            print(f"   ‚úó {col_name} (n√£o encontrada no CSV)")

    # Combina as colunas de data e hora em uma √∫nica coluna de timestamp
    try:
        # Garante que a hora est√° no formato HH:MM
        df['HORA'] = df['HORA'].str.slice(0, 2) + ':' + df['HORA'].str.slice(2, 4)
        # Converte a data do formato DD/MM/YYYY para YYYY/MM/DD
        df['timestamp'] = pd.to_datetime(df['DATA'] + ' ' + df['HORA'], format='%d/%m/%Y %H:%M')
    except Exception as e:
        print(f"‚úó Erro ao converter data e hora. Verifique o formato no CSV. Erro: {e}")
        return

    df.sort_values('timestamp', inplace=True)

    # --- VERIFICA√á√ÉO DE DUPLICATAS ---
    first_timestamp = df['timestamp'].min()
    last_timestamp = df['timestamp'].max()
    print(f"üìÖ Per√≠odo dos dados: {first_timestamp} at√© {last_timestamp}")

    print("üîç Verificando registros existentes...")
    # Verificar se j√° existem dados para este per√≠odo
    for sensor_id in list(cleaned_sensor_map.values())[:1]:  # Verificar apenas um sensor para economizar tempo
        duplicate_count = db_manager.check_duplicate_records(sensor_id, first_timestamp, last_timestamp)
        if duplicate_count > 0:
            response = input(f"‚ö†Ô∏è  Encontrados {duplicate_count} registros existentes para o sensor {sensor_id} neste per√≠odo. Continuar mesmo assim? (s/n): ")
            if response.lower() != 's':
                print("Opera√ß√£o cancelada pelo usu√°rio.")
                return
        break

    # --- INSER√á√ÉO NO BANCO DE DADOS ---

    total_records = len(df)
    print(f"üíæ Preparando inser√ß√£o de {total_records} linhas de dados hor√°rios no banco de dados.")

    # Preparar dados para inser√ß√£o em lote
    insert_data = []

    # Query de inser√ß√£o
    insert_query = """
    INSERT INTO registro_leituras
    (ID_Sensor, Valor_Leitura, Timestamp_Leitura, Qualidade)
    VALUES (%s, %s, %s, %s)
    """

    # Contar registros processados
    records_processed = 0
    records_skipped = 0
    batch_size = 500  # Reduzir tamanho do lote para melhor estabilidade

    print("üöÄ Iniciando processamento dos dados...")

    # Itera sobre cada linha do dataframe
    for index, row in df.iterrows():
        timestamp_str = row['timestamp'].strftime('%Y-%m-%d %H:%M:%S')

        # Itera sobre o nosso mapa de sensores para criar um INSERT para cada leitura
        for col_name, sensor_id in cleaned_sensor_map.items():
            if col_name in df.columns:
                valor_leitura = row[col_name]

                # S√≥ insere se o valor da leitura for v√°lido (n√£o for NaN)
                if pd.notna(valor_leitura):
                    insert_data.append((sensor_id, float(valor_leitura), timestamp_str, 'Confiavel'))
                else:
                    records_skipped += 1

        # Inserir em lotes para melhor performance
        if len(insert_data) >= batch_size:
            rows_inserted = db_manager.execute_batch_insert(insert_query, insert_data)
            records_processed += rows_inserted
            print(f"‚úì Inseridos {records_processed} registros no banco de dados...")
            insert_data = []  # Limpar o lote

        # Mostrar progresso a cada 50 linhas processadas
        if (index + 1) % 50 == 0:
            progress = ((index + 1) / total_records) * 100
            print(f"üìä Progresso: {progress:.1f}% ({index + 1}/{total_records} linhas processadas)")

    # Inserir registros restantes
    if insert_data:
        rows_inserted = db_manager.execute_batch_insert(insert_query, insert_data)
        records_processed += rows_inserted

    print(f"\nüéâ Inser√ß√£o conclu√≠da com sucesso!")
    print(f"‚úÖ Registros inseridos: {records_processed}")
    print(f"‚è≠Ô∏è  Registros ignorados (valores nulos): {records_skipped}")
    print(f"üóÑÔ∏è  Banco: {DB_CONFIG['database']}")
    print(f"üåê Host: {DB_CONFIG['host']}:{DB_CONFIG['port']}")
    print("‚úÖ Todos os dados foram importados com sucesso!")

if __name__ == "__main__":
    # Verificar se o m√≥dulo mysql.connector est√° instalado
    try:
        import mysql.connector
    except ImportError:
        print("‚úó ERRO: O m√≥dulo 'mysql-connector-python' n√£o est√° instalado.")
        print("Para instalar, execute: pip install mysql-connector-python")
        sys.exit(1)

    # Verificar se o pandas est√° instalado
    try:
        import pandas as pd
    except ImportError:
        print("‚úó ERRO: O m√≥dulo 'pandas' n√£o est√° instalado.")
        print("Para instalar, execute: pip install pandas")
        sys.exit(1)

    print("=" * 60)
    print("  üå¶Ô∏è  IMPORTADOR DE DADOS INMET PARA BANCO STOMGUARD")
    print("=" * 60)
    print(f"Iniciando processamento em {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # Executar o processamento e inser√ß√£o
    processar_dados_e_inserir_no_banco()

    print("=" * 60)
    print(f"Processamento finalizado em {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
